{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4519c97e-5a3b-4040-8b21-9b5209712cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: 02_build_macro_and_parquet.ipynb\n",
    "\n",
    "import pandas as pd, numpy as np, json\n",
    "from pathlib import Path\n",
    "\n",
    "# I/O\n",
    "OUTDIR = Path(\"macro_retrieval\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base CSV from Step 1 (your cleaned retrieval-ready file)\n",
    "BASE_CSV = OUTDIR = \"base_sp500_2007_2023.csv\"   # <- change if your filename differs\n",
    "\n",
    "# Switch to True if you want to fetch via FRED API now\n",
    "USE_FRED = False   # otherwise expects four small CSVs (see Cell 3)\n",
    "\n",
    "# Helper: business-day index (NYSE-style approximated by pandas)\n",
    "def bdays(start, end):\n",
    "    return pd.bdate_range(start=start, end=end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8852f4ce-83cf-4c7a-b38e-0be29ec7b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train window: 2007-08-07 → 2023-07-14\n",
      "Base rows: 2799 Columns: ['Date', 'Movement', 'Open', 'Close_lag1', 'High_lag1', 'Volume_lag1', 'Daily_Return_lag1', 'Volatility_lag1', 'sentiment_volatility_lag1', 'aggregate_sentiment_score_lag1', 'text_embed']\n"
     ]
    }
   ],
   "source": [
    "base = pd.read_csv(BASE_CSV, parse_dates=[\"Date\"]).sort_values(\"Date\")\n",
    "\n",
    "# Parse the JSON string embedding -> numpy array\n",
    "def parse_emb(s):\n",
    "    arr = np.array(json.loads(s), dtype=np.float32)\n",
    "    return arr\n",
    "\n",
    "base[\"text_embed\"] = base[\"prev_day_embedding_json\"].apply(parse_emb)\n",
    "base = base.drop(columns=[\"prev_day_embedding_json\"])\n",
    "\n",
    "# Derive train window from the data (no hard-coded dates)\n",
    "TRAIN_START = base[\"Date\"].min().normalize()\n",
    "TRAIN_END   = base[\"Date\"].max().normalize()\n",
    "print(\"Train window:\", TRAIN_START.date(), \"→\", TRAIN_END.date())\n",
    "\n",
    "# Quick sanity checks\n",
    "assert base[\"Date\"].is_monotonic_increasing, \"Dates not sorted\"\n",
    "assert base[\"Movement\"].isin([0,1]).all(), \"Movement must be 0/1\"\n",
    "print(\"Base rows:\", len(base), \"Columns:\", list(base.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e635f7e-c1e3-4858-ab26-91ece277aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch macro from: 2006-05-23 to: 2023-07-14\n"
     ]
    }
   ],
   "source": [
    "# For CPI YoY we need 12 prior months; add cushion for publication lags (max ~30d for GDP)\n",
    "train_start = pd.to_datetime(TRAIN_START)\n",
    "train_end   = pd.to_datetime(TRAIN_END)\n",
    "\n",
    "macro_fetch_start = (train_start - pd.DateOffset(months=13) - pd.DateOffset(days=45)).normalize()\n",
    "macro_fetch_end   = train_end\n",
    "print(\"Fetch macro from:\", macro_fetch_start.date(), \"to:\", macro_fetch_end.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9eb460d-818b-4d48-927b-706993e5ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred = Fred(api_key=\"ae61356225c8e8de915564116aa0c9f3\")\n",
    "\n",
    "# CPI (monthly): compute YoY %\n",
    "cpi = fred.get_series('CPIAUCSL', observation_start=macro_fetch_start, observation_end=macro_fetch_end)\n",
    "cpi = cpi.to_frame(\"cpi\").reset_index().rename(columns={\"index\":\"Date\"})\n",
    "cpi[\"cpi_yoy\"] = cpi[\"cpi\"].pct_change(12) * 100\n",
    "\n",
    "# Unemployment (monthly, %)\n",
    "unrate = fred.get_series('UNRATE', observation_start=macro_fetch_start, observation_end=macro_fetch_end)\n",
    "unrate = unrate.to_frame(\"unrate\").reset_index().rename(columns={\"index\":\"Date\"})\n",
    "\n",
    "# 10Y-2Y spread (daily, pp)\n",
    "t10y2y = fred.get_series('T10Y2Y', observation_start=macro_fetch_start, observation_end=macro_fetch_end)\n",
    "t10y2y = t10y2y.to_frame(\"t10y2y\").reset_index().rename(columns={\"index\":\"Date\"})\n",
    "\n",
    "# Real GDP (quarterly): compute QoQ %\n",
    "gdp = fred.get_series('GDPC1', observation_start=macro_fetch_start, observation_end=macro_fetch_end)\n",
    "gdp = gdp.to_frame(\"gdp\").reset_index().rename(columns={\"index\":\"Date\"})\n",
    "gdp[\"gdp_qoq\"] = gdp[\"gdp\"].pct_change() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4f01e1-71ce-4eb0-b468-7b500d9dabbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide macro rows (b-days): 4474\n"
     ]
    }
   ],
   "source": [
    "# Merge sparse macro series (native frequencies)\n",
    "macro = (cpi[[\"Date\",\"cpi_yoy\"]]\n",
    "         .merge(unrate[[\"Date\",\"unrate\"]], on=\"Date\", how=\"outer\")\n",
    "         .merge(t10y2y[[\"Date\",\"t10y2y\"]], on=\"Date\", how=\"outer\")\n",
    "         .merge(gdp[[\"Date\",\"gdp_qoq\"]], on=\"Date\", how=\"outer\")\n",
    "        ).sort_values(\"Date\")\n",
    "\n",
    "# Reindex to business days on the wide window, forward-fill across days\n",
    "wide_idx = bdays(macro_fetch_start, macro_fetch_end)\n",
    "macro_w = (macro.set_index(\"Date\").reindex(wide_idx).rename_axis(\"Date\").ffill())\n",
    "\n",
    "# Apply publication lags (strict causality)\n",
    "macro_w[\"cpi_yoy_lagged\"] = macro_w[\"cpi_yoy\"].shift(10)  # CPI +10d\n",
    "macro_w[\"unrate_lagged\"]  = macro_w[\"unrate\"].shift(5)    # UNRATE +5d\n",
    "macro_w[\"t10y2y_lagged\"]  = macro_w[\"t10y2y\"]             # 0d\n",
    "macro_w[\"gdp_qoq_lagged\"] = macro_w[\"gdp_qoq\"].shift(30)  # GDP +30d\n",
    "\n",
    "macro_w = macro_w.ffill().reset_index()\n",
    "print(\"Wide macro rows (b-days):\", len(macro_w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57b322a-3195-4739-8d2b-5a8003e09940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro (train) rows: 4159 Cols: ['Date', 'cpi_yoy_lagged_z', 'unrate_lagged_z', 't10y2y_lagged_z', 'gdp_qoq_lagged_z']\n"
     ]
    }
   ],
   "source": [
    "# Trim to actual training window\n",
    "macro_train = macro_w[(macro_w[\"Date\"] >= TRAIN_START) & (macro_w[\"Date\"] <= TRAIN_END)].copy()\n",
    "\n",
    "lag_cols = [\"cpi_yoy_lagged\",\"unrate_lagged\",\"t10y2y_lagged\",\"gdp_qoq_lagged\"]\n",
    "mu = macro_train[lag_cols].mean()\n",
    "sd = macro_train[lag_cols].std(ddof=0).replace(0, 1.0)\n",
    "\n",
    "for c in lag_cols:\n",
    "    macro_train[c + \"_z\"] = (macro_train[c] - mu[c]) / sd[c]\n",
    "\n",
    "macro_daily = macro_train[[\"Date\"] + [c+\"_z\" for c in lag_cols]]\n",
    "print(\"Macro (train) rows:\", len(macro_daily), \"Cols:\", macro_daily.columns.tolist())\n",
    "\n",
    "# Save stats for reproducibility\n",
    "json.dump({\"mu\": mu.to_dict(), \"sd\": sd.to_dict()}, open(\"macro_zscore_stats.json\",\"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87a9238-bd95-441b-b0ce-cc627659afc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sp500_features.parquet\n",
      "Rows: 2799\n",
      "Columns: ['Date', 'Movement', 'Open', 'Close_lag1', 'High_lag1', 'Volume_lag1', 'Daily_Return_lag1', 'Volatility_lag1', 'sentiment_volatility_lag1', 'aggregate_sentiment_score_lag1', 'text_embed', 'cpi_yoy_lagged_z', 'unrate_lagged_z', 't10y2y_lagged_z', 'gdp_qoq_lagged_z']\n"
     ]
    }
   ],
   "source": [
    "# Keep base within the same train window (safety)\n",
    "base_train = base[(base[\"Date\"] >= TRAIN_START) & (base[\"Date\"] <= TRAIN_END)].copy()\n",
    "\n",
    "final = base_train.merge(macro_daily, on=\"Date\", how=\"left\").sort_values(\"Date\")\n",
    "\n",
    "# Parquet prefers lists (not ndarrays) for variable-length vectors\n",
    "final[\"text_embed\"] = final[\"text_embed\"].apply(lambda a: a.tolist())\n",
    "\n",
    "final_path = \"sp500_features.parquet\"\n",
    "final.to_parquet(final_path, index=False)\n",
    "\n",
    "print(\"Saved:\", final_path)\n",
    "print(\"Rows:\", len(final))\n",
    "print(\"Columns:\", list(final.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b5cf42-eecc-45c0-b725-f732f899b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2799\n",
      "Date range: 2007-08-07 → 2023-07-14\n",
      "\n",
      "NaNs per macro z-col:\n",
      " cpi_yoy_lagged_z    0\n",
      "unrate_lagged_z     0\n",
      "t10y2y_lagged_z     0\n",
      "gdp_qoq_lagged_z    0\n",
      "dtype: int64\n",
      "\n",
      "Embedding first row dim: 384\n",
      "Embeddings: OK (all dims == 384)\n",
      "\n",
      "Movement unique values: [0 1]\n",
      "Movement: OK (only 0/1)\n",
      "\n",
      "Date monotonic increasing: True\n",
      "Duplicates: OK (no duplicate dates)\n"
     ]
    }
   ],
   "source": [
    "# === Sanity prints (no asserts) ===\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Rows: {len(final)}\")\n",
    "print(f\"Date range: {final['Date'].min().date()} → {final['Date'].max().date()}\")\n",
    "\n",
    "# 1) NaNs in required macro z-cols\n",
    "required = [\"cpi_yoy_lagged_z\",\"unrate_lagged_z\",\"t10y2y_lagged_z\",\"gdp_qoq_lagged_z\"]\n",
    "nan_counts = final[required].isna().sum()\n",
    "print(\"\\nNaNs per macro z-col:\\n\", nan_counts)\n",
    "\n",
    "# 2) Embedding dimensionality & consistency\n",
    "first_dim = len(final[\"text_embed\"].iloc[0])\n",
    "dims = final[\"text_embed\"].apply(lambda v: len(v))\n",
    "print(f\"\\nEmbedding first row dim: {first_dim}\")\n",
    "if dims.nunique() == 1 and dims.iloc[0] == first_dim:\n",
    "    print(f\"Embeddings: OK (all dims == {first_dim})\")\n",
    "else:\n",
    "    print(\"Embeddings: Inconsistencies found!\")\n",
    "    print(\"Dim counts:\", dims.value_counts().to_dict())\n",
    "    bad_idx = dims[dims != first_dim].index[:5]\n",
    "    print(\"Sample problematic indices:\", list(bad_idx))\n",
    "\n",
    "# 3) Movement integrity\n",
    "valid_mask = final[\"Movement\"].isin([0, 1])\n",
    "invalid_count = (~valid_mask).sum()\n",
    "print(\"\\nMovement unique values:\", np.sort(final[\"Movement\"].unique()))\n",
    "if invalid_count == 0:\n",
    "    print(\"Movement: OK (only 0/1)\")\n",
    "else:\n",
    "    print(f\"Movement: {invalid_count} invalid rows detected.\")\n",
    "    print(\"Sample invalid row indices:\", list(final.index[~valid_mask][:5]))\n",
    "\n",
    "# 4) Date monotonicity\n",
    "is_mono = final[\"Date\"].is_monotonic_increasing\n",
    "print(\"\\nDate monotonic increasing:\", is_mono)\n",
    "if not is_mono:\n",
    "    deltas = final[\"Date\"].diff()\n",
    "    bad = deltas.dt.days.fillna(0) < 0\n",
    "    print(\"First monotonic violations at indices:\", list(final.index[bad][:5]))\n",
    "\n",
    "# 5) Duplicate dates\n",
    "dup_mask = final[\"Date\"].duplicated(keep=False)\n",
    "dup_count = int(dup_mask.sum())\n",
    "if dup_count == 0:\n",
    "    print(\"Duplicates: OK (no duplicate dates)\")\n",
    "else:\n",
    "    print(f\"Duplicates: Found {dup_count} duplicate rows.\")\n",
    "    dup_dates = final.loc[dup_mask, \"Date\"].value_counts().head(5)\n",
    "    print(\"Sample duplicate dates and counts:\\n\", dup_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb02527-dbaa-481c-b599-9b31279bfa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved joint + meta. Shapes: (2799, 384) (2799, 388)\n",
      "FAISS index built. Vectors: 2799 Dim: 388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, json\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(\"macro_retrieval\")\n",
    "train_path = \"sp500_features.parquet\"\n",
    "final = pd.read_parquet(train_path)\n",
    "\n",
    "# Stack train matrices\n",
    "text_train  = np.vstack(final[\"text_embed\"].to_numpy()).astype(\"float32\")              # [N, d]\n",
    "macro_train = final[[\"cpi_yoy_lagged_z\",\"unrate_lagged_z\",\"t10y2y_lagged_z\",\"gdp_qoq_lagged_z\"]].to_numpy().astype(\"float32\")  # [N, 4]\n",
    "\n",
    "# Joint = norm([ text ; α * macro ])\n",
    "alpha = 0.5\n",
    "joint_train = np.concatenate([text_train, alpha * macro_train], axis=1)\n",
    "joint_train /= (np.linalg.norm(joint_train, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "# Save artifacts\n",
    "np.save(OUTDIR/\"text_train.npy\", text_train)\n",
    "pd.DataFrame({\"date\": final[\"Date\"], \"joint_embed\": list(joint_train)}).to_parquet(OUTDIR/\"train_joint_vectors.parquet\", index=False)\n",
    "final[[\"Date\"]].rename(columns={\"Date\":\"date\"}).to_parquet(OUTDIR/\"index_meta.parquet\", index=False)\n",
    "print(\"Saved joint + meta. Shapes:\", text_train.shape, joint_train.shape)\n",
    "\n",
    "# Build FAISS index\n",
    "try:\n",
    "    import faiss\n",
    "except Exception as e:\n",
    "    print(\"FAISS not found. Install with: pip install faiss-cpu\")\n",
    "    raise\n",
    "\n",
    "d = joint_train.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index = faiss.IndexIDMap2(index)\n",
    "ids = np.arange(joint_train.shape[0]).astype(\"int64\")\n",
    "index.add_with_ids(joint_train.astype(\"float32\"), ids)\n",
    "faiss.write_index(index, str(OUTDIR/\"index_flat_ip.faiss\"))\n",
    "print(\"FAISS index built. Vectors:\", index.ntotal, \"Dim:\", d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207681ed-f235-4b4b-b80f-e09fdc0e2b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
